"""
rag_service.py â€” æœ¬åœ°è½»é‡çº§ RAG æ¨¡å—ï¼ˆå›½å†…æº+ç¼“å­˜ï¼‰
Author: Code GPT ğŸ§ 
"""

import os
import numpy as np
from modelscope import snapshot_download
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# === 1ï¸âƒ£ ç»Ÿä¸€é¡¹ç›®è·¯å¾„ ===
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
MODEL_DIR = os.path.join(BASE_DIR, "models")
DATA_DIR = os.path.join(BASE_DIR, "data")
CACHE_DIR = os.path.join(BASE_DIR, "cache")

for d in [MODEL_DIR, DATA_DIR, CACHE_DIR]:
    os.makedirs(d, exist_ok=True)

DATA_PATH = os.path.join(DATA_DIR, "campus_knowledge.txt")
EMBED_PATH = os.path.join(CACHE_DIR, "embeddings.npy")

# === 2ï¸âƒ£ æ¨¡å‹åŠ è½½ï¼ˆä½¿ç”¨ ModelScope é•œåƒï¼‰ ===
print("âš™ï¸ æ­£åœ¨åŠ è½½è½»é‡è¯­ä¹‰æ¨¡å‹ (MiniLM-L12-v2, æ¥è‡ªé­”å¡”)...")

model_path = snapshot_download(
    model_id="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    cache_dir=MODEL_DIR
)

embedder = SentenceTransformer(model_path)

# === 3ï¸âƒ£ åŠ è½½çŸ¥è¯†åº“ ===
def load_knowledge():
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"âŒ çŸ¥è¯†æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        text = f.read()
    chunks = [c.strip() for c in text.split("\n\n") if c.strip()]
    return chunks

knowledge_chunks = load_knowledge()

# === 4ï¸âƒ£ Embedding ç¼“å­˜ ===
if os.path.exists(EMBED_PATH):
    print("ğŸ“‚ æ£€æµ‹åˆ°ç¼“å­˜ embeddingsï¼Œæ­£åœ¨åŠ è½½ ...")
    embeddings = np.load(EMBED_PATH)
else:
    print("âš™ï¸ æ­£åœ¨ç”ŸæˆçŸ¥è¯†å‘é‡ï¼ˆæœ¬åœ°CPUè®¡ç®—ï¼‰...")
    embeddings = embedder.encode(knowledge_chunks)
    np.save(EMBED_PATH, embeddings)
    print(f"ğŸ’¾ å‘é‡å·²ç¼“å­˜è‡³: {EMBED_PATH}")

print(f"âœ… å·²åŠ è½½ {len(embeddings)} æ¡çŸ¥è¯†ç‰‡æ®µã€‚")

# === 5ï¸âƒ£ ç›¸ä¼¼åº¦æ£€ç´¢ ===
def search_relevant_knowledge(query: str, top_k: int = 2):
    query_vec = embedder.encode([query])
    sims = cosine_similarity(query_vec, embeddings)[0]
    top_indices = np.argsort(sims)[::-1][:top_k]
    return [knowledge_chunks[i] for i in top_indices]
